# Улучшение бинарной классификации и тестирование нелинейных моделей

В ходе защиты проекта, комиссией было подмечено, что в ходе получения наиболее оптимальных моделей ML было бы лучше максимизировать метрику `PR-AUC` и подбирать порог вероятности для максимизации метрики `F-1`, а не просто подбирать параметры максимизируя метрику `F-1`.

## Подбор порогов для уже протестированных линейных моделей

Во время работы над бейзлайном были протестированы модели классификации `LogisticRegression` и `LinearSVC`. Они на тот момент с подбором и без подбора параметров, но без подбора порогов вероятности показали следующие результаты на тестовых данных:

| Model                         | Data        | Precision | Recall | F-1  | AUC  | PR-AUC |
|--------------------------------|------------|-----------|--------|------|------|--------|
| Logistic Regression Base       | BoW Test   | 0.75      | 0.48   | 0.58 | 0.91 | 0.68   |
| Logistic Regression Optuna     | BoW Test   | 0.51      | 0.80   | 0.63 | 0.92 | 0.67   |
| Logistic Regression Base       | TF-IDF Test| 0.78      | 0.48   | 0.59 | 0.93 | 0.72   |
| Logistic Regression Optuna     | TF-IDF Test| 0.48      | 0.82   | 0.61 | 0.93 | 0.71   |
| Linear SVC Base                | BoW Test   | 0.71      | 0.48   | 0.57 | 0.89 | 0.65   |
| Linear SVC Optuna              | BoW Test   | 0.53      | 0.78   | 0.63 | 0.92 | 0.66   |
| Linear SVC Base                | TF-IDF Test| 0.79      | 0.48   | 0.60 | 0.93 | 0.72   |
| Linear SVC Optuna              | TF-IDF Test| 0.50      | 0.80   | 0.61 | 0.93 | 0.71   |

Видно, что в плане метрики `F-1`, наилучшие результаты выдают модели, которые были обучены на BoW данных. Однако, если обучать модели и подбирать пороги вероятности класса, то результаты немного меняются:

| Model                         | P-threshold | Data        | Precision | Recall | F-1  | AUC  | PR-AUC |
|--------------------------------|------------|------------|-----------|--------|------|------|--------|
| Logistic Regression Base       | 0.19       | BoW Test   | 0.62      | 0.68   | 0.65 | 0.91 | 0.68   |
| Logistic Regression Optuna     | 0.21       | BoW Test   | 0.63      | 0.66   | 0.65 | 0.91 | 0.68   |
| Logistic Regression Base       | 0.24       | TF-IDF Test| 0.63      | 0.67   | 0.65 | 0.93 | 0.72   |
| Logistic Regression Optuna     | 0.24       | TF-IDF Test| 0.63      | 0.68   | 0.65 | 0.93 | 0.72   |
| Linear SVC Base                | 0.19       | BoW Test   | 0.60      | 0.67   | 0.63 | 0.90 | 0.67   |
| Linear SVC Optuna              | 0.18       | BoW Test   | 0.62      | 0.68   | 0.65 | 0.92 | 0.69   |
| Linear SVC Base                | 0.23       | TF-IDF Test| 0.64      | 0.68   | 0.66 | 0.93 | 0.72   |
| Linear SVC Optuna              | 0.22       | TF-IDF Test| 0.63      | 0.69   | 0.66 | 0.93 | 0.73   |

Как можно заметить после подбора порогов вероятности класса, метрики у всех моделей стали относительно похожими, а метрика `F-1` повысилась на 5-12% в зависимости от модели и подбора параметров. Однако, если рассматривать совокупность метрик `F-1`, `PR-AUC` и `AUC`, то оказывается, что лучшими моделями, являются модели, которые были обучены на TF-IDF данных. В частности, лучшей моделью является `LinearSVC` обученная на TF-IDF данных с подобранными гиперпараметрами в `optuna`.

## Подбор порогов для XGBoost

В ходе работы над бейзлайном уже использовалась нелинейная модель градиентного бустинга `XGBoostClassifier`. Данная модель показала следующие результаты при обучении с подбором и без подбора параметров на BoW данных:

| Model                     | Data      | Precision | Recall | F-1  | AUC  | PR-AUC |
|---------------------------|----------|-----------|--------|------|------|--------|
| XGBoostClassifier Base    | BoW Test | 0.81      | 0.35   | 0.49 | 0.90 | 0.66   |
| XGBoostClassifier Optuna  | BoW Test | 0.65      | 0.65   | 0.65 | 0.92 | 0.71   |

На TF-IDF данных модель не удалось обучить из-за нехватки ОЗУ. Однако к текущему чекпоинту все-таки удалось обучить эту модель оптимизируя процессы. Результат получился следующий:

| Model                     | P-threshold | Data      | Precision | Recall | F-1  | AUC  | PR-AUC |
|---------------------------|------------|----------|-----------|--------|------|------|--------|
| XGBoostClassifier Base    | 0.19       | BoW Test | 0.61      | 0.63   | 0.62 | 0.90 | 0.66   |
| XGBoostClassifier Optuna  | 0.59       | BoW Test | 0.63      | 0.68   | 0.66 | 0.92 | 0.71   |
| XGBoostClassifier Base    | 0.22       | TF-IDF Test | 0.64  | 0.63   | 0.64 | 0.90 | 0.69   |
| XGBoostClassifier Optuna  | 0.25       | TF-IDF Test | 0.64  | 0.68   | 0.66 | 0.93 | 0.73   |

Как можно заметить, в итоге удалось получить результат чуть лучше, чем на этапе бейзлайна с подбором параметров. По итогу заметен прирост в метрике `F-1` на 1.5%, если сравнивать с предыдущей лучшей моделью обученной на данных BoW. При этом всем, по совокупности метрик `F-1`, `PR-AUC` и `AUC`, лучшей моделью оказался `XGBoostClassifier` обученный на TF-IDF данных с подобранными гиперпараметрами в `optuna`. Стоит отметить, что разница между лучшими моделями `LinearSVC` и `XGBoostClassifier` крайне мала.

## Тестирование других нелинейных моделей основанных на деревьях

Основной целью данного чекпоинта является тестирование нелинейных моделей, которые не использовались в рамках чекпоинта 3. Начнем с моделей основанных на деревьях. В качестве модели решающего древа была взята реализация из `sklearn` `DecisionTreeClassifier`, а в качестве реализации случайного леса была взята реализация из `XGBoost` `XGBoostRFClassifier`, так как реализация из `sklearn` плохо умеет работать со sparse матрицами, а это наш основной тип для обучения и получения предсказаний на данных. Результаты были следующими:

| Model                          | P-threshold | Data          | Precision | Recall | F-1  | AUC  | PR-AUC |
|--------------------------------|------------|--------------|-----------|--------|------|------|--------|
| DecisionTreeClassifier Base    | 0.37       | BoW Train    | 0.99      | 1.00   | 1.00 | 1.00 | 1.00   |
| DecisionTreeClassifier Base    | 0.37       | BoW Test     | 0.52      | 0.51   | 0.51 | 0.72 | 0.53   |
| DecisionTreeClassifier Optuna  | 0.24       | BoW Train    | 0.49      | 0.58   | 0.53 | 0.82 | 0.56   |
| DecisionTreeClassifier Optuna  | 0.24       | BoW Test     | 0.50      | 0.58   | 0.54 | 0.81 | 0.56   |
| DecisionTreeClassifier Base    | 0.37       | TF-IDF Train | 0.99      | 1.00   | 0.99 | 1.00 | 1.00   |
| DecisionTreeClassifier Base    | 0.37       | TF-IDF Test  | 0.55      | 0.50   | 0.53 | 0.72 | 0.54   |
| DecisionTreeClassifier Optuna  | 0.29       | TF-IDF Train | 0.59      | 0.64   | 0.62 | 0.84 | 0.65   |
| DecisionTreeClassifier Optuna  | 0.29       | TF-IDF Test  | 0.58      | 0.62   | 0.60 | 0.83 | 0.63   |
| XGBoostRFClassifier Base       | 0.11       | BoW Train    | 0.76      | 0.25   | 0.38 | 0.62 | 0.55   |
| XGBoostRFClassifier Base       | 0.11       | BoW Test     | 0.78      | 0.24   | 0.36 | 0.61 | 0.55   |
| XGBoostRFClassifier Optuna     | 0.19       | BoW Train    | 0.60      | 0.63   | 0.62 | 0.89 | 0.65   |
| XGBoostRFClassifier Optuna     | 0.19       | BoW Test     | 0.60      | 0.62   | 0.61 | 0.88 | 0.64   |
| XGBoostRFClassifier Base       | 0.13       | TF-IDF Train | 0.77      | 0.23   | 0.36 | 0.61 | 0.55   |
| XGBoostRFClassifier Base       | 0.13       | TF-IDF Test  | 0.79      | 0.22   | 0.34 | 0.61 | 0.56   |
| XGBoostRFClassifier Optuna     | 0.18       | TF-IDF Train | 0.58      | 0.65   | 0.62 | 0.89 | 0.66   |
| XGBoostRFClassifier Optuna     | 0.18       | TF-IDF Test  | 0.59      | 0.65   | 0.62 | 0.88 | 0.66   |

В таблицу включены метрики как для тренировочного, так и для тестового датасета. Как можно заметить `DecisionTreeClassifier` при обучении без подбора параметров страдает от переобучения, выдавая идеальные метрики на `train` и ужасные метрики на `test`. Подбор параметров решает данную проблему. Метрики после подбора выравниваются и становятся примерно одинаковыми как для `train`, так и для `test`. 
Случай для случайного леса и `XGBoostRFClassifier` обратный. Модель как на `train`, так и на `test` данных, выдает довольно плохой результат по метрикам. Данная проблема также решается подбором гиперпараметров. По итогу с подобранными параметрами случайный лес выдает результат чуть лучше, чем древо решений. Причем лучшие метрики получаются снова при обучении TF-IDF данных. Однако ни одна из этих моделей не превзошла результатов моделей `LinearSVC` и `XGBoostClassifier` по метрикам.

## Тестирование SVM моделей с нелинейным ядром

Перейдем к SVM моделям с нелинейными ядрами. В частности, к моделям с полиномиальным, гауссовским и сигмоидным ядром. Для обучения данных моделей использовалась библиотека `thundersvm`. Данная библиотека позволяет обучать модели SVM более быстро, чем в библиотеке `sklearn`. Например, в ходе бейзлайна были попытки обучить базовый `SVC` с линейным ядром, но даже спустя 16 часов модель могла обучиться. Из-за этого применялся `LinearSVC`, который может обучиться достаточно быстро благодаря оптимизации. Однако `LinearSVC`, как можно судить по названию, работает только с линейным ядром. Библиотека `thundersvm`, несмотря на сложности в ее установке, позволила обучать модели SVM с нелинейными ядрами, примерно за 2-4 часа, что относительно быстро, но недостаточно быстро, чтобы подбирать параметры для моделей. Также были проблемы с обучением SVM моделей на BoW данных. Поэтому для обучения использовались только TF-IDF данные. Результат можно увидеть на таблице:

| Model                         | P-threshold | Data         | Precision | Recall | F-1  | AUC  | PR-AUC |
|--------------------------------|------------|-------------|-----------|--------|------|------|--------|
| SVC(kernel = 'polynomial')     | 0.01       | TF-IDF Train | 0.11      | 1.00   | 0.20 | 0.50 | 0.55   |
| SVC(kernel = 'polynomial')     | 0.01       | TF-IDF Test  | 0.12      | 1.00   | 0.21 | 0.50 | 0.56   |
| SVC(kernel = 'rbf')            | 0.09       | TF-IDF Train | 0.58      | 0.72   | 0.64 | 0.91 | 0.67   |
| SVC(kernel = 'rbf')            | 0.09       | TF-IDF Test  | 0.58      | 0.71   | 0.64 | 0.91 | 0.67   |
| SVC(kernel = 'sigmoid')        | 0.10       | TF-IDF Train | 0.62      | 0.66   | 0.64 | 0.91 | 0.67   |
| SVC(kernel = 'sigmoid')        | 0.10       | TF-IDF Test  | 0.62      | 0.66   | 0.64 | 0.90 | 0.67   |

По таблице можно увидеть, что `SVC` с полиномиальным ядром показывает себя довольно плохо как на обучающей, так и на тестовой выборках. Рассматривая метрику `AUC` для этой модели, можно сказать, что она больше похожа на модель случайного угадывания и она почти не различает классы. Говоря про `SVC` с гауссовским и сигмоидным ядром, можно сказать, что они довольно похожи по метрикам и не дотягивают до уровня метрик базового `Linear SVC` по тому же `F-1` на 3%.

## LinearSVC + TF-IDF + Hashing

Все основные модели нелинейные модели, которые можно было бы применить закончились, и теперь хотелось бы рассказать про совмещение методов векторизации текста. В частности про склейку данных обработанных `HashingVectorizer` и `TfidfVectorizer` из `sklearn`. Такая склейка из преобразованных данных может дать потенциальный прирост в метриках моделей. Для примера, рассмотрим обучение модели `LinearSVC` на таких данных:

| Model              | P-threshold | Data                  | Precision | Recall | F-1  | AUC  | PR-AUC |
|--------------------|------------|-----------------------|-----------|--------|------|------|--------|
| Linear SVC Optuna | 0.22       | TF-IDF + Hash Train   | 0.64      | 0.71   | 0.68 | 0.95 | 0.75   |
| Linear SVC Optuna | 0.22       | TF-IDF + Hash Test    | 0.64      | 0.70   | 0.66 | 0.94 | 0.74   |

В целом такое совмещение обработки данных дает относительно неплохой прирост в качестве модели `Linear SVC`.  `AUC` и `PR-AUC` стали выше примерно на 1%, что относительно неплохой прирост в качестве. Возможно еще больший бы прирост в качестве дало бы использование модели `XGBoost` на таких данных. Однако были попытки обучить такую модель на этих данных, но даже с учетом всех оптимизаций, ее не удалось обучить из-за ограничений в ОЗУ машины, на которой производилась работа.

## Вывод

После тестирования всех или почти всех классических моделей машинного обучения можно сделать следующие выводы:

- Среди всех протестированных моделей наилучшими являются `LinearSVC` и `XGBoost`, которые были протестированны еще на этапе бейзлайна
- Подбор параметров по `PR-AUC` и подбор порогов вероятности класса значительно улучшают метрики моделей
- Новые нелинейные модели, которые были протестированы в рамках этого чекпоинта не дали какого-либо прироста в метриках для моделей
- На текущий момент наилучшими метриками полученными являются `F-1 = 0.66`, `AUC = 0.94` и `PR-AUC = 0.74`, которые удалось получить с помощью модели `LinearSVC` и экспериментальных данных **TF-IDF + Hash**
- Вероятно модель `XGBoost` может себя лучше показать на экспериментальных данных, но к сожалению ограничения мощностей не позволяют это проверить с оптимальным диапазоном параметров
