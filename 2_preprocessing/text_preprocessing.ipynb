{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном ноутбуке представлена обновленная предобработка текста для обучения/инференса.\n",
    "\n",
    "Этот вариант обработки включает в себя:\n",
    "- удаление ссылок\n",
    "- лемматизацию\n",
    "- замену распространенных масок ругательств на базовые формы\n",
    "- очистку пунктуации, которая определяет эмоциональную пунктуацию (например, смайлы и подряд идущие восклицательные знаки)\n",
    "- исправление опечаток\n",
    "- удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.mask_patterns = {\n",
    "            re.compile(r'[f4$][u*µùúûü][c\\(k<][k%]+', re.IGNORECASE): 'fuck',\n",
    "            re.compile(r'[5$][h#][i1!][t+7]+', re.IGNORECASE): 'shit',\n",
    "            re.compile(r'a[s$]{2}h[o0]l[e3]', re.IGNORECASE): 'asshole',\n",
    "            re.compile(r'[b8][i1!][7+][c\\(][h4]+', re.IGNORECASE): 'bitch',\n",
    "            re.compile(r'\\b[a@4][s$5]{2,}\\b', re.IGNORECASE): 'ass',\n",
    "            re.compile(r'\\bp[1!i]ss\\b', re.IGNORECASE): 'piss',\n",
    "            re.compile(r'\\b[d][a@4][m][nñ]+\\b', re.IGNORECASE): 'damn',\n",
    "            re.compile(r'\\b[c\\(][uµùúûü][nñ][t7+]+\\b', re.IGNORECASE): 'cunt',\n",
    "            re.compile(r'[p][0oöø][r][nñ]+', re.IGNORECASE): 'porn',\n",
    "            re.compile(r'\\b[w][h#][0oöø][r][e3€]+\\b', re.IGNORECASE): 'whore',\n",
    "            re.compile(r'\\b[n][i1!][g6][g6][e3€][r®]+\\b', re.IGNORECASE): 'nigger',\n",
    "            re.compile(r'\\b[f4$][a@4][g6][s$5]+\\b', re.IGNORECASE): 'fags',\n",
    "            re.compile(r'\\b[s$5][l1!][uµùúûü][t7+]+\\b', re.IGNORECASE): 'slut',\n",
    "            re.compile(r'\\b[d][1!][c\\(][k%]+\\b', re.IGNORECASE): 'dick',\n",
    "            re.compile(r'\\b[b8][0oöø][0oöø][b8]+\\b', re.IGNORECASE): 'boob',\n",
    "            re.compile(r'\\b[c\\(][0oöø][c\\(][k%]+\\b', re.IGNORECASE): 'cock',\n",
    "            re.compile(r'\\bb[o0]ll[o0]cks\\b', re.IGNORECASE): 'bollocks',\n",
    "            re.compile(r'\\b[p][uµùúûü][s$5][s$5][y¥]+\\b', re.IGNORECASE): 'pussy',\n",
    "            re.compile(r'\\b[m][0oöø][f4$][0oöø]+\\b', re.IGNORECASE): 'mofo',\n",
    "            re.compile(r'\\b[t7+][w][a@4][t7+]+\\b', re.IGNORECASE): 'twat',\n",
    "            re.compile(r'\\b[b8][a@4][s$5][t7+][a@4][r®][d]+\\b', re.IGNORECASE): 'bastard',\n",
    "            re.compile(r'\\b[m][0oöø][t7+][h#][e3€][r®][f4$][uµùúûü][c$$][k%][e3€][r®]+\\b', re.IGNORECASE): 'motherfucker',\n",
    "            re.compile(r'\\bw[a@]nk[e3]r\\b', re.IGNORECASE): 'wanker'\n",
    "        }\n",
    "\n",
    "        self.swear_words = set(self.mask_patterns.values())\n",
    "        self.emoticon_pattern = re.compile(\n",
    "            r\"([:=;]-?[)(\\]\\[DdPp/\\\\|]|<3|[!?]{2,})\", \n",
    "            flags=re.VERBOSE|re.IGNORECASE\n",
    "        )\n",
    "        self.url_re = re.compile(r'(https?://\\S+|www\\.\\S+)', re.IGNORECASE)\n",
    "        \n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        self.stopwords = self.nlp.Defaults.stop_words\n",
    "\n",
    "        self.sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "        self.sym_spell.load_dictionary(\n",
    "            'frequency_dictionary_en_82_765.txt',\n",
    "            encoding='utf-8-sig',\n",
    "            term_index=0,\n",
    "            count_index=1\n",
    "        )\n",
    "    \n",
    "    def lemmatize_text(self, text: str):\n",
    "        doc = self.nlp(text)\n",
    "        processed_parts = []\n",
    "        for token in doc:\n",
    "            processed_parts.append(token.lemma_.lower())\n",
    "            if token.whitespace_:\n",
    "                processed_parts.append(' ')\n",
    "\n",
    "        processed_text = ''.join(processed_parts)\n",
    "        del processed_parts\n",
    "\n",
    "        return processed_text\n",
    "    \n",
    "    def correct_swear_words(self, text: str):\n",
    "        for pattern, base in self.mask_patterns.items():\n",
    "            text = pattern.sub(base, text)\n",
    "        return text\n",
    "    \n",
    "    def clear_punct(self, text: str):\n",
    "        emoticons = self.emoticon_pattern.findall(text)\n",
    "        keep_chars = set(''.join(emoticons))\n",
    "        translator = str.maketrans(\n",
    "            {char: ' ' for char in string.punctuation + '\\n\\t\\r' if char not in keep_chars}\n",
    "        )\n",
    "        text = text.translate(translator)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    def correct_typos(self, text: str):\n",
    "        doc = self.nlp(text)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if not token.is_alpha:\n",
    "                tokens.append(token.text)\n",
    "                continue\n",
    "\n",
    "            word = token.text\n",
    "            suggestions = self.sym_spell.lookup(word, Verbosity.TOP)\n",
    "            if suggestions and suggestions[0].term != word:\n",
    "                word = suggestions[0].term\n",
    "            if word not in self.stopwords:\n",
    "                tokens.append(word)\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def process_text(self, text: str):        \n",
    "        # Удаление ссылок\n",
    "        processed_text = self.url_re.sub('', text.lower())\n",
    "\n",
    "        # Лемматизация текста\n",
    "        processed_text = self.lemmatize_text(processed_text)\n",
    "        \n",
    "        # Замена замаскированных ругательств\n",
    "        processed_text = self.correct_swear_words(processed_text)\n",
    "        \n",
    "        # Очистка пунктуации (кроме эмотиконов)\n",
    "        processed_text = self.clear_punct(processed_text)\n",
    "\n",
    "        # Исправление опечаток и удаление стоп-слов\n",
    "        processed_text = self.correct_typos(processed_text)\n",
    "        \n",
    "        return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = pd.read_csv('/Users/lev_k/HSE/YP/modified_train.csv', index_col=0).shape[0]\n",
    "gc.collect()\n",
    "\n",
    "processor = TextProcessor()\n",
    "reader = pd.read_csv('/Users/lev_k/HSE/YP/modified_train.csv', chunksize=200000)\n",
    "with tqdm(total=total_rows, desc=\"Processing\") as pbar:\n",
    "    for i, chunk in enumerate(reader):\n",
    "        chunk['processed_ct'] = chunk['comment_text'].parallel_apply(processor.process_text)\n",
    "        chunk.to_parquet(f'processed_chunk_{i}.parquet', index=False)\n",
    "\n",
    "        pbar.update(len(chunk))\n",
    "\n",
    "        del chunk\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "processed_df = pd.concat(\n",
    "    [pd.read_parquet(f, engine='pyarrow') for f in glob.glob('processed_chunk_*.parquet')],\n",
    "    ignore_index=False\n",
    ")\n",
    "processed_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv('preprocessed_train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
