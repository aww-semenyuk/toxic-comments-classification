# Эксперименты с Word2Vec

## Мотивация

При обучении бейзлайна мы использовали базовый подход к формированию матрицы объект-признак `bag-of-words`, а также его улучшение (за счет учета важности токенов) в виде `tf-idf`. Оба этих подхода имеют существенный с точки зрения логики недостаток - они не учитывают поряд слов в предложении. Поэтому следующим шагом мы попытаемся улучшить качество классификации за счет обучения модели `Word2Vec`, которая позволяет получить латентные представления токенов с учетом их последовательности.

## Результаты

[Notebook](/3_1_BASELINE/word2vec.ipynb)

| Word2Vec | Embedding strategy | Estimator | f1 score (Validation) |
| --- | --- | --- | --- |
| Pretrained (word2vec-google-news-300) | Averaging | LogisticRegression | 0.28 |
| Pretrained (word2vec-google-news-300) | Tf-Idf | LogisticRegression | 0.41 |
| Default params (CBoW) | Tf-Idf | LogisticRegression | 0.30 |
| Default params (SkipGram) | Tf-Idf | LogisticRegression | 0.33 |
| Tuned (SkipGram) | Tf-Idf | LogisticRegression | 0.42 |
| Tuned (SkipGram) | Tf-Idf | XGBoost (default params) | 0.41 |
| Tuned (SkipGram) | Tf-Idf | XGBoost (tuned) | 0.47 |

1\. В качестве бейзлайна использовали pretrained эмбеддинги `word2vec-google-news-300` (3 млн токенов размерности 300). Также по ним сделали вывод о лучшей стратегии получения эмбеддингов текстов по эмбеддингам входящих в них токенов - взвешивание на `tf-idf` меру дает заметно лучший результат, чем простое усреднение векторов. 

2\. Пробуем обучить свою модель, используя библитеку [gensim](https://pypi.org/project/gensim/), с дефолтными параметрами, для двух алгоритмов: `Continuous Bag of Words` (попытка предсказать текущее слово по контексту) и `SkipGram` (попытка предсказать контекст по текущему слову). Качество обоих моделей получилось заметно хуже, чем при использовании pretrained эмбеддингов, при этом `SkipGram` показал лучшие метрики по сравнению с `CBoW`, поэтому при дальнейшем подборе параметров будем использовать именно этот алгоритм.

3\. Подбираем гиперпараметры модели, удалось немного побить результат pretrained.

4\. Пробуем другой эстиматор (градиентный бустинг вместо логистической регрессии), удалось еще немного улучшить метрики.

5\. К сожалению, лучшая (по валидации) модель оказалась переобученной, а также показала метрики хуже, чем базовые модели. В дальнейшем можно пробовать бороться с переобучением, уменьшая сложность эстиматора или W2V модели, однако побить метрики базовых моделей, судя по всему, не удасться, так что возможно имеет смысл сразу перейти к более сложным нейросетевым подходам.
